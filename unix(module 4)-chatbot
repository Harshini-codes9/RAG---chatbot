
# -*- coding: utf-8 -*-
"""chatbot12.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15L0h_tn4M3qmy0JfHD5FbU8mYxzIltcM
"""

!pip install -q pymupdf sentence-transformers chromadb transformers accelerate

from google.colab import files
uploaded = files.upload()

import fitz  # PyMuPDF

pdf_path = "/content/18CS56_Module 4 notes (1) (1).pdf"
doc = fitz.open(pdf_path)

texts = []
for page in doc:
    texts.append(page.get_text())

doc.close()
print("PDF read successfully ✅")

def split_text_into_chunks(texts, max_chunk_size=250):
    chunks = []
    for text in texts:
        words = text.split()
        current = ""

        for word in words:
            if len(current) + len(word) <= max_chunk_size:
                current += word + " "
            else:
                chunks.append(current.strip())
                current = word + " "

        if current:
            chunks.append(current.strip())

    print(f"{len(chunks)} chunks created ✅")
    return chunks

chunks = split_text_into_chunks(texts)

from sentence_transformers import SentenceTransformer

embed_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = embed_model.encode(chunks).tolist()

print("Embeddings created ✅")

import chromadb

chroma_client = chromadb.Client()
collection = chroma_client.get_or_create_collection(name="OS_RAG")

for i in range(len(chunks)):
    collection.add(
        ids=[str(i)],
        documents=[chunks[i]],
        embeddings=[embeddings[i]]
    )

print("All chunks stored in ChromaDB ✅")

question = input("Ask your question: ")

results = collection.query(
    query_texts=[question],
    n_results=4
)

retrieved_docs = results["documents"][0]
context = " ".join(retrieved_docs)

print("\nRetrieved Context:\n", context)

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-small")
model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-small")

prompt = f"""
You are an Operating Systems expert.
Answer the question ONLY using the context below.

Context:
{context}

Question:
{question}
"""

inputs = tokenizer(prompt, return_tensors="pt", truncation=True)

outputs = model.generate(
    inputs["input_ids"],
    max_length=200,
    temperature=0.7
)

print("\nGenerated Answer:\n")
print(tokenizer.decode(outputs[0], skip_special_tokens=True))



















